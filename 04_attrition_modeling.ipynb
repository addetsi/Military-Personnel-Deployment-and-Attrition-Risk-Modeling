{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6448c91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.6.0)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.11.0)\n",
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.14.1-py3-none-any.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from lightgbm) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from lightgbm) (1.17.0)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.4.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.8.0)\n",
      "Collecting sklearn-compat<0.2,>=0.1.5 (from imbalanced-learn)\n",
      "  Downloading sklearn_compat-0.1.5-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: joblib<2,>=1.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from imbalanced-learn) (3.6.0)\n",
      "Downloading imbalanced_learn-0.14.1-py3-none-any.whl (235 kB)\n",
      "   ---------------------------------------- 0.0/235.4 kB ? eta -:--:--\n",
      "   ------ --------------------------------- 41.0/235.4 kB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 153.6/235.4 kB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 235.4/235.4 kB 2.0 MB/s eta 0:00:00\n",
      "Downloading sklearn_compat-0.1.5-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: sklearn-compat, imbalanced-learn\n",
      "  Attempting uninstall: imbalanced-learn\n",
      "    Found existing installation: imbalanced-learn 0.11.0\n",
      "    Uninstalling imbalanced-learn-0.11.0:\n",
      "      Successfully uninstalled imbalanced-learn-0.11.0\n",
      "Successfully installed imbalanced-learn-0.14.1 sklearn-compat-0.1.5\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm -U imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9819f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully!\n",
      "Project root: C:\\Users\\hp\\Desktop\\gaf\\Ghana-Armed-Forces-Personnel-Deployment-and-Attrition-Risk-Modeling\n",
      "Random seed: 42\n",
      "Target sample size: 1000 personnel\n",
      "Libraries imported successfully\n",
      "XGBoost version: 3.0.0\n",
      "LightGBM version: 4.6.0\n"
     ]
    }
   ],
   "source": [
    "#import libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Ml libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, \n",
    "    roc_auc_score, roc_curve, precision_recall_curve,\n",
    "    f1_score, recall_score, precision_score\n",
    ") \n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import shap \n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import config\n",
    "\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(config.RANDOM_SEED)\n",
    "\n",
    "print(\"Libraries imported successfully\")\n",
    "print(f\"XGBoost version: {xgb.__version__}\")\n",
    "print(f\"LightGBM version: {lgb.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68beba50",
   "metadata": {},
   "source": [
    "# Load Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b2979ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded: (1000, 75)\n",
      "\n",
      "Tartfet distribution: \n",
      "attrition_risk\n",
      "LOW_RISK       726\n",
      "MEDIUM_RISK    211\n",
      "HIGH_RISK       63\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Target distribution (%)\n",
      "attrition_risk\n",
      "LOW_RISK       72.6%\n",
      "MEDIUM_RISK    21.1%\n",
      "HIGH_RISK       6.3%\n",
      "Name: proportion, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#load dataset\n",
    "data_path = config.PROCESSED_DATA_DIR/config.FEATURES_ENGINEERED_FILE\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"Data loaded: {df.shape}\")\n",
    "print(f\"\\nTartfet distribution: \")\n",
    "print(df['attrition_risk'].value_counts())\n",
    "print(f\"\\nTarget distribution (%)\")\n",
    "print(df[\"attrition_risk\"].value_counts(normalize=True).apply(lambda x: f\"{x:.1%}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbec2d26",
   "metadata": {},
   "source": [
    "# Data Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb2ebe72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (1000, 73)\n",
      "Target shape: (1000,)\n",
      "\n",
      "Categorical features to encode: 12\n",
      "['gender', 'service_branch', 'rank', 'MOS', 'marital_status', 'education_level', 'mental_health_status', 'deployment_type', 'combat_exposure_level', 'financial_stress_indicator', 'relocation_willingness', 'performance_trajectory']\n",
      "\n",
      "Features after encoding: (1000, 100)\n",
      "Added 27 dummy variables\n"
     ]
    }
   ],
   "source": [
    "#sepetate features and targets\n",
    "X = df.drop(['attrition_risk', 'readiness_score'], axis=1)\n",
    "y = df['attrition_risk']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "#encode categorical features \n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "print(f\"\\nCategorical features to encode: {len(categorical_features)}\")\n",
    "print(categorical_features)\n",
    "\n",
    "#one-hot encode categorical features \n",
    "X_encoded = pd.get_dummies(X, columns= categorical_features, drop_first=True)\n",
    "\n",
    "print(f\"\\nFeatures after encoding: {X_encoded.shape}\")\n",
    "print(f\"Added {X_encoded.shape[1] - X.shape[1]} dummy variables\")\n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9205ea0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target encoding:\n",
      "HIGH_RISK: 0\n",
      "LOW_RISK: 1\n",
      "MEDIUM_RISK: 2\n"
     ]
    }
   ],
   "source": [
    "#encode target variables (string numeric)\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "#store mapping \n",
    "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "print(\"Target encoding:\")\n",
    "for label, code in label_mapping.items():\n",
    "    print(f\"{label}: {code}\")\n",
    "    \n",
    "#HIGH_RISK is encoded as 0 for easier recall calculation \n",
    "#remap \n",
    "\n",
    "if label_mapping['HIGH_RISK'] != 0:\n",
    "    print(\"\\n⚠️  Remapping labels so HIGH_RISK = 0 (for recall optimization)\")\n",
    "    label_encoder.classes_ = np.array(['HIGH_RISK', 'LOW_RISK', 'MEDIUM_RISK'])\n",
    "    y_encoded = label_encoder.transform(y)\n",
    "    label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "    print(\"New mapping:\")\n",
    "    for label, code in label_mapping.items():\n",
    "        print(f\"  {label}: {code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "636b95fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splits:\n",
      "  Training: 600 samples (60.0%)\n",
      "  Validation: 200 samples (20.0%)\n",
      "  Test: 200 samples (20.0%)\n",
      "\n",
      "Class distribution in splits:\n",
      "  Training: [ 37 436 127]\n",
      "  Validation: [ 13 145  42]\n",
      "  Test: [ 13 145  42]\n"
     ]
    }
   ],
   "source": [
    "#train/val/test split (60/20/20 stratified)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X_encoded, y_encoded, \n",
    "    test_size=config.TEST_SIZE, \n",
    "    stratify=y_encoded, \n",
    "    random_state=config.RANDOM_SEED\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, \n",
    "    test_size=config.VAL_SIZE, \n",
    "    stratify=y_temp, \n",
    "    random_state=config.RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(\"Dataset splits:\")\n",
    "print(f\"  Training: {X_train.shape[0]} samples ({X_train.shape[0]/len(df):.1%})\")\n",
    "print(f\"  Validation: {X_val.shape[0]} samples ({X_val.shape[0]/len(df):.1%})\")\n",
    "print(f\"  Test: {X_test.shape[0]} samples ({X_test.shape[0]/len(df):.1%})\")\n",
    "\n",
    "# Check stratification\n",
    "print(\"\\nClass distribution in splits:\")\n",
    "print(f\"  Training: {np.bincount(y_train)}\")\n",
    "print(f\"  Validation: {np.bincount(y_val)}\")\n",
    "print(f\"  Test: {np.bincount(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bf3d3e",
   "metadata": {},
   "source": [
    "# Handling Class Imbalance with SMOTE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3180a56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before smote\n",
      "class 0 (HIGH_RISK): 37\n",
      " class 1 (HIGH_RISK): 436\n",
      " class 2 (MEDIUM_RISK): 127\n",
      "\n",
      "Class distribution AFTER SMOTE:\n",
      "  Class 0 (HIGH_RISK): 436\n",
      "  Class 1 (LOW_RISK): 436\n",
      "  Class 2 (MEDIUM_RISK): 436\n",
      "\n",
      "Training set increased from 600 to 1308 samples\n"
     ]
    }
   ],
   "source": [
    "print(\"Class distribution before smote\")\n",
    "print(f\"class 0 (HIGH_RISK): {np.sum(y_train == 0)}\")\n",
    "print(f\" class 1 (HIGH_RISK): {np.sum(y_train == 1)}\")\n",
    "print(f\" class 2 (MEDIUM_RISK): {np.sum(y_train == 2)}\")\n",
    "\n",
    "#appky smote to balance classes \n",
    "smote = SMOTE(random_state=config.RANDOM_SEED)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"\\nClass distribution AFTER SMOTE:\")\n",
    "print(f\"  Class 0 (HIGH_RISK): {np.sum(y_train_balanced == 0)}\")\n",
    "print(f\"  Class 1 (LOW_RISK): {np.sum(y_train_balanced == 1)}\")\n",
    "print(f\"  Class 2 (MEDIUM_RISK): {np.sum(y_train_balanced == 2)}\")\n",
    "print(f\"\\nTraining set increased from {len(y_train)} to {len(y_train_balanced)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4081dd",
   "metadata": {},
   "source": [
    "# Feature Scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92d0bb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features scaled using standardscaler\n",
      "Total features: 100\n"
     ]
    }
   ],
   "source": [
    "#standardize features \n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_balanced)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#store feature names \n",
    "feature_names = X_encoded.columns.tolist()\n",
    "\n",
    "print(f\"Features scaled using standardscaler\")\n",
    "print(f\"Total features: {len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba8491a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
